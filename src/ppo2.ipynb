{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ***Parallel PPO 2***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Sequence\n",
    "\n",
    "import distrax\n",
    "import flax.linen as nn\n",
    "import gymnax\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import optax\n",
    "from flax.linen.initializers import constant, orthogonal\n",
    "from flax.training.train_state import TrainState\n",
    "from gymnax.wrappers.purerl import FlattenObservationWrapper, LogWrapper\n",
    "from dataclasses import dataclass\n",
    "import time\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "\n",
    "from utils import Transition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCritic(nn.Module):\n",
    "    action_dim: Sequence[int]\n",
    "    activation: str = \"tanh\"\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        if self.activation == \"relu\":\n",
    "            activation = nn.relu\n",
    "        else:\n",
    "            activation = nn.tanh\n",
    "        actor_mean = nn.Dense(\n",
    "            64, kernel_init=orthogonal(np.sqrt(2)), bias_init=constant(0.0)\n",
    "        )(x)\n",
    "        actor_mean = activation(actor_mean)\n",
    "        actor_mean = nn.Dense(\n",
    "            64, kernel_init=orthogonal(np.sqrt(2)), bias_init=constant(0.0)\n",
    "        )(actor_mean)\n",
    "        actor_mean = activation(actor_mean)\n",
    "        actor_mean = nn.Dense(\n",
    "            self.action_dim, kernel_init=orthogonal(0.01), bias_init=constant(0.0)\n",
    "        )(actor_mean)\n",
    "        pi = distrax.Categorical(logits=actor_mean)\n",
    "\n",
    "        critic = nn.Dense(\n",
    "            64, kernel_init=orthogonal(np.sqrt(2)), bias_init=constant(0.0)\n",
    "        )(x)\n",
    "        critic = activation(critic)\n",
    "        critic = nn.Dense(\n",
    "            64, kernel_init=orthogonal(np.sqrt(2)), bias_init=constant(0.0)\n",
    "        )(critic)\n",
    "        critic = activation(critic)\n",
    "        critic = nn.Dense(1, kernel_init=orthogonal(1.0), bias_init=constant(0.0))(\n",
    "            critic\n",
    "        )\n",
    "\n",
    "        return pi, jnp.squeeze(critic, axis=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Args:\n",
    "    seed: int = 0\n",
    "    save_model: bool = False\n",
    "    log_results: bool = False\n",
    "\n",
    "    wandb_project_name: str = \"improved-gradient-steps\"\n",
    "    wandb_entity: str = \"rpegoud\"\n",
    "    logging_dir: str = \".\"\n",
    "\n",
    "    # Algorithm specific arguments\n",
    "    trainer: str = \"base_ppo\"\n",
    "    env_name: str = \"CartPole-v1\"\n",
    "    total_timesteps: int = 5e4\n",
    "    learning_rate: float = 2.5e-4\n",
    "    n_agents: int = 16\n",
    "    num_envs: int = 4\n",
    "    num_steps: int = 128\n",
    "    update_epochs: int = 4\n",
    "    num_minibatches: int = 4\n",
    "    gamma: float = 0.99\n",
    "    gae_lambda: float = 0.95\n",
    "    clip_eps: float = 0.2\n",
    "    ent_coef: float = 0.01\n",
    "    vf_coef: float = 0.5\n",
    "    max_grad_norm: float = 0.5\n",
    "    alpha: float = 0.2\n",
    "    activation: str = \"tanh\"\n",
    "    anneal_lr: bool = True\n",
    "    debug: bool = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Args()\n",
    "NUM_UPDATES = args.total_timesteps // args.num_steps // args.num_envs\n",
    "MINIBATCH_SIZE = args.num_envs * args.num_steps // args.num_minibatches\n",
    "env, env_params = gymnax.make(args.env_name)\n",
    "env = FlattenObservationWrapper(env)\n",
    "env = LogWrapper(env)\n",
    "rng = jax.random.PRNGKey(0)\n",
    "\n",
    "def linear_schedule(count):\n",
    "    frac = 1.0 - (count // (args.num_minibatches * args.update_epochs)) / NUM_UPDATES\n",
    "    return args.learning_rate * frac\n",
    "\n",
    "\n",
    "# INIT NETWORK\n",
    "network = ActorCritic(env.action_space(env_params).n, activation=args.activation)\n",
    "rng, _rng = jax.random.split(rng)\n",
    "init_x = jnp.zeros(env.observation_space(env_params).shape)\n",
    "network_params = network.init(_rng, init_x)\n",
    "if args.anneal_lr:\n",
    "    tx = optax.chain(\n",
    "        optax.clip_by_global_norm(args.max_grad_norm),\n",
    "        optax.adam(learning_rate=linear_schedule, eps=1e-5),\n",
    "    )\n",
    "else:\n",
    "    tx = optax.chain(\n",
    "        optax.clip_by_global_norm(args.max_grad_norm),\n",
    "        optax.adam(args.learning_rate, eps=1e-5),\n",
    "    )\n",
    "train_state = TrainState.create(\n",
    "    apply_fn=network.apply,\n",
    "    params=network_params,\n",
    "    tx=tx,\n",
    ")\n",
    "\n",
    "# INIT ENV\n",
    "rng, _rng = jax.random.split(rng)\n",
    "reset_rng = jax.random.split(_rng, args.num_envs)\n",
    "obsv, env_state = jax.vmap(env.reset, in_axes=(0, None))(reset_rng, env_params)\n",
    "rng, _rng = jax.random.split(rng)\n",
    "runner_state = (train_state, env_state, obsv, _rng)\n",
    "# TRAIN LOOP\n",
    "\n",
    "\n",
    "def _env_step(runner_state, unused):\n",
    "    \"\"\"\n",
    "    Steps the environment across ``num_envs``.\n",
    "    Returns the updated runner state and observation.\n",
    "    \"\"\"\n",
    "    train_state, env_state, last_obs, rng = runner_state\n",
    "\n",
    "    # SELECT ACTION\n",
    "    rng, _rng = jax.random.split(rng)\n",
    "    pi, value = network.apply(train_state.params, last_obs)\n",
    "    actions = pi.sample(seed=_rng)\n",
    "    log_prob = pi.log_prob(actions)\n",
    "\n",
    "    # STEP ENV\n",
    "    rng, _rng = jax.random.split(rng)\n",
    "    rng_step = jax.random.split(_rng, args.num_envs)\n",
    "    obsv, env_state, reward, done, info = jax.vmap(env.step, in_axes=(0, 0, 0, None))(\n",
    "        rng_step, env_state, actions, env_params\n",
    "    )\n",
    "    transition = Transition(done, actions, value, reward, log_prob, last_obs, info)\n",
    "    runner_state = (train_state, env_state, obsv, rng)\n",
    "    return runner_state, transition\n",
    "\n",
    "\n",
    "runner_state, traj_batch = jax.lax.scan(_env_step, runner_state, None, args.num_steps)\n",
    "\n",
    "# CALCULATE ADVANTAGE\n",
    "train_state, env_state, last_obs, rng = runner_state\n",
    "# get the last value estimate to initialize gae computation\n",
    "_, last_val = network.apply(train_state.params, last_obs)\n",
    "\n",
    "\n",
    "def _calculate_gae(traj_batch, last_val):\n",
    "    \"\"\"\n",
    "    Compute the generalized advantage estimation of a trajectory batch.\n",
    "    \"\"\"\n",
    "\n",
    "    def _get_advantages(gae_and_next_value, transition):\n",
    "        \"\"\"\n",
    "        Iteratively computes the GAE starting from the last transition.\n",
    "        Uses `lax.scan` to carry the current (`gae`, `next_value`) tuple\n",
    "        while iterating through transitions.\n",
    "        \"\"\"\n",
    "        gae, next_value = gae_and_next_value\n",
    "        done, value, reward = (\n",
    "            transition.done,\n",
    "            transition.value,\n",
    "            transition.reward,\n",
    "        )\n",
    "        # td-error\n",
    "        delta = reward + args.gamma * next_value * (1 - done) - value\n",
    "        # generalized advantage in recursive form\n",
    "        gae = delta + args.gamma * args.gae_lambda * (1 - done) * gae\n",
    "        return (gae, value), gae  # (carry_over), collected results\n",
    "\n",
    "    _, advantages = jax.lax.scan(\n",
    "        _get_advantages,\n",
    "        (jnp.zeros_like(last_val), last_val),\n",
    "        traj_batch,\n",
    "        # gae is computed backwards as the advantage at time t\n",
    "        # depends on the estimated advantages of future timesteps\n",
    "        reverse=True,\n",
    "        # unrolls the loop body of the scan operation 16 iterations at a time\n",
    "        # enables the 128 steps (default value) to be completed in 8 iterations\n",
    "        unroll=16,\n",
    "    )\n",
    "    return advantages, advantages + traj_batch.value\n",
    "\n",
    "advantages, targets = _calculate_gae(traj_batch, last_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'params': {'Dense_0': {'bias': (64,), 'kernel': (4, 64)},\n",
       "  'Dense_1': {'bias': (64,), 'kernel': (64, 64)},\n",
       "  'Dense_2': {'bias': (2,), 'kernel': (64, 2)},\n",
       "  'Dense_3': {'bias': (64,), 'kernel': (4, 64)},\n",
       "  'Dense_4': {'bias': (64,), 'kernel': (64, 64)},\n",
       "  'Dense_5': {'bias': (1,), 'kernel': (64, 1)}}}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def _loss_fn(params, traj_batch, gae, targets):\n",
    "    # RERUN NETWORK\n",
    "    pi, value = network.apply(params, traj_batch.obs)\n",
    "    log_prob = pi.log_prob(traj_batch.action)\n",
    "\n",
    "    # CALCULATE VALUE LOSS\n",
    "    value_pred_clipped = traj_batch.value + (value - traj_batch.value).clip(\n",
    "        -args.clip_eps, args.clip_eps\n",
    "    )\n",
    "    value_losses = jnp.square(value - targets)\n",
    "    value_losses_clipped = jnp.square(value_pred_clipped - targets)\n",
    "    value_loss = 0.5 * jnp.maximum(value_losses, value_losses_clipped).mean()\n",
    "\n",
    "    # CALCULATE ACTOR LOSS\n",
    "    ratio = jnp.exp(log_prob - traj_batch.log_prob)\n",
    "    gae = (gae - gae.mean()) / (gae.std() + 1e-8)\n",
    "    loss_actor1 = ratio * gae\n",
    "    loss_actor2 = (\n",
    "        jnp.clip(\n",
    "            ratio,\n",
    "            1.0 - args.clip_eps,\n",
    "            1.0 + args.clip_eps,\n",
    "        )\n",
    "        * gae\n",
    "    )\n",
    "    loss_actor = -jnp.minimum(loss_actor1, loss_actor2)\n",
    "    loss_actor = loss_actor.mean()\n",
    "    entropy = pi.entropy().mean()\n",
    "\n",
    "    total_loss = loss_actor + args.vf_coef * value_loss - args.ent_coef * entropy\n",
    "    return total_loss, (value_loss, loss_actor, entropy)\n",
    "\n",
    "\n",
    "grad_fn = jax.value_and_grad(_loss_fn, has_aux=True)\n",
    "total_loss, grads = grad_fn(train_state.params, traj_batch, advantages, targets)\n",
    "train_state = train_state.apply_gradients(grads=grads)\n",
    "jax.tree_map(lambda x: x.shape, grads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "\n",
    "def _loss_fn(params, traj_batch, gae, targets):\n",
    "    pi, value = network.apply(params, traj_batch.obs)\n",
    "    log_prob = pi.log_prob(traj_batch.action)\n",
    "\n",
    "    value_pred_clipped = traj_batch.value + (value - traj_batch.value).clip(\n",
    "        -args.clip_eps, args.clip_eps\n",
    "    )\n",
    "    value_losses = jnp.square(value - targets)\n",
    "    value_losses_clipped = jnp.square(value_pred_clipped - targets)\n",
    "    value_loss = 0.5 * jnp.maximum(value_losses, value_losses_clipped)\n",
    "\n",
    "    ratio = jnp.exp(log_prob - traj_batch.log_prob)\n",
    "    gae = (gae - gae.mean()) / (gae.std() + 1e-8)\n",
    "    loss_actor1 = ratio * gae\n",
    "    loss_actor2 = (\n",
    "        jnp.clip(\n",
    "            ratio,\n",
    "            1.0 - args.clip_eps,\n",
    "            1.0 + args.clip_eps,\n",
    "        )\n",
    "        * gae\n",
    "    )\n",
    "    loss_actor = -jnp.minimum(loss_actor1, loss_actor2)\n",
    "\n",
    "    entropy = pi.entropy()\n",
    "\n",
    "    return value_loss, loss_actor, entropy\n",
    "\n",
    "\n",
    "# Vectorize the loss function to compute per-sample gradients\n",
    "_loss_fn_vmap = jax.vmap(_loss_fn, in_axes=(None, 0, 0, 0))\n",
    "\n",
    "\n",
    "# Wrapper function to compute total loss and individual losses\n",
    "def compute_losses_and_grads(params, traj_batch, gae, targets):\n",
    "    value_losses, actor_losses, entropies = _loss_fn_vmap(\n",
    "        params, traj_batch, gae, targets\n",
    "    )\n",
    "\n",
    "    total_losses = (\n",
    "        actor_losses + args.vf_coef * value_losses - args.ent_coef * entropies\n",
    "    )\n",
    "\n",
    "    value_grads = jax.grad(\n",
    "        lambda p: jnp.sum(_loss_fn_vmap(p, traj_batch, gae, targets)[0])\n",
    "    )(params)\n",
    "    actor_grads = jax.grad(\n",
    "        lambda p: jnp.sum(_loss_fn_vmap(p, traj_batch, gae, targets)[1])\n",
    "    )(params)\n",
    "    entropy_grads = jax.grad(\n",
    "        lambda p: jnp.sum(_loss_fn_vmap(p, traj_batch, gae, targets)[2])\n",
    "    )(params)\n",
    "\n",
    "    return (\n",
    "        total_losses,\n",
    "        value_losses,\n",
    "        actor_losses,\n",
    "        entropies,\n",
    "        value_grads,\n",
    "        actor_grads,\n",
    "        entropy_grads,\n",
    "    )\n",
    "\n",
    "\n",
    "(\n",
    "    total_losses,\n",
    "    value_losses,\n",
    "    actor_losses,\n",
    "    entropies,\n",
    "    value_grads,\n",
    "    actor_grads,\n",
    "    entropy_grads,\n",
    ") = compute_losses_and_grads(train_state.params, traj_batch, advantages, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "\n",
    "def _loss_fn(params, traj_batch, gae, targets):\n",
    "    # RERUN NETWORK\n",
    "    pi, value = network.apply(params, traj_batch.obs)\n",
    "    log_prob = pi.log_prob(traj_batch.action)\n",
    "\n",
    "    # CALCULATE VALUE LOSS\n",
    "    value_pred_clipped = traj_batch.value + (value - traj_batch.value).clip(\n",
    "        -args.clip_eps, args.clip_eps\n",
    "    )\n",
    "    value_losses = jnp.square(value - targets)\n",
    "    value_losses_clipped = jnp.square(value_pred_clipped - targets)\n",
    "    value_loss = 0.5 * jnp.maximum(value_losses, value_losses_clipped)\n",
    "\n",
    "    # CALCULATE ACTOR LOSS\n",
    "    ratio = jnp.exp(log_prob - traj_batch.log_prob)\n",
    "    gae = (gae - gae.mean()) / (gae.std() + 1e-8)\n",
    "    loss_actor1 = ratio * gae\n",
    "    loss_actor2 = (\n",
    "        jnp.clip(\n",
    "            ratio,\n",
    "            1.0 - args.clip_eps,\n",
    "            1.0 + args.clip_eps,\n",
    "        )\n",
    "        * gae\n",
    "    )\n",
    "    loss_actor = -jnp.minimum(loss_actor1, loss_actor2)\n",
    "    entropy = pi.entropy()\n",
    "\n",
    "    return value_loss, loss_actor, entropy\n",
    "\n",
    "\n",
    "def compute_per_sample_grads(params, traj_batch, gae, targets):\n",
    "    value_losses, actor_losses, entropies = _loss_fn_vmap(\n",
    "        params, traj_batch, gae, targets\n",
    "    )\n",
    "\n",
    "    def total_loss_fn(p):\n",
    "        # per sample losses\n",
    "        value_losses, actor_losses, entropies = jax.vmap(\n",
    "            _loss_fn, in_axes=(None, 0, 0, 0)\n",
    "        )(p, traj_batch, gae, targets)\n",
    "        \n",
    "        total_losses = (\n",
    "            actor_losses + args.vf_coef * value_losses - args.ent_coef * entropies\n",
    "        )\n",
    "        return total_losses.sum(), (value_losses, actor_losses, entropies)\n",
    "\n",
    "    grad_fn = jax.value_and_grad(total_loss_fn, has_aux=True)\n",
    "    (_, (value_losses, actor_losses, entropies)), total_grads = grad_fn(params)\n",
    "\n",
    "    value_grads = jax.grad(lambda p: jnp.sum(value_losses))(params)\n",
    "    actor_grads = jax.grad(lambda p: jnp.sum(actor_losses))(params)\n",
    "    entropy_grads = jax.grad(lambda p: jnp.sum(entropies))(params)\n",
    "\n",
    "    return (\n",
    "        total_grads,\n",
    "        value_losses,\n",
    "        actor_losses,\n",
    "        entropies,\n",
    "        value_grads,\n",
    "        actor_grads,\n",
    "        entropy_grads,\n",
    "    )\n",
    "\n",
    "\n",
    "(\n",
    "    total_grads,\n",
    "    value_losses,\n",
    "    actor_losses,\n",
    "    entropies,\n",
    "    value_grads,\n",
    "    actor_grads,\n",
    "    entropy_grads,\n",
    ") = compute_per_sample_grads(train_state.params, traj_batch, advantages, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "chex.assert_trees_all_equal_shapes_and_dtypes(value_grads, actor_grads, entropy_grads)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prioritized-sampling-and-gradient-steps-nM5uJI4I-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
